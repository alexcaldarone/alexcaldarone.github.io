<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> MCMC for Gaussian Mixture Models | Alex John Caldarone </title> <meta name="author" content="Alex John Caldarone"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alexcaldarone.github.io/blog/2025/gmm-mcmc/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alex </span> John  Caldarone </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume_website.pdf">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">MCMC for Gaussian Mixture Models</h1> <p class="post-meta"> February 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics,</a>   <a href="/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian_Statistics</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Gaussian Mixture Models (GMMs) are incredibly versatile tools for solving real-world problems, from clustering and image analysis to speech processing. They enable us to model complex, multimodal data distributions effectively. Traditionally, the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" rel="external nofollow noopener" target="_blank">Expectation-Maximization algorithm</a> is used to estimate the parameters of these models.</p> <p>But what happens if we shift to a <em>Bayesian approach?</em> In a Bayesian setting, we gain the advantage of incorporating prior knowledge and quantifying uncertainty, but this comes at the cost of more complex computations. In order to compute the quantities of interest, we turn to <strong>Markov Chain Monte Carlo (MCMC)</strong> methods, which allow us to sample from the posterior distribution of the model’s parameters.</p> <p>Recently I implemented a completed a project on comparing different MCMC (Markov Chain Monte Carlo) samplers when estimating the parameters of Hierarchical Guassian Mixture Model, and thought I’d share some details! You can find the code of the project <a href="https://github.com/alexcaldarone/gmm-mcmc" rel="external nofollow noopener" target="_blank">here</a>. In a nutshell, it allows the user to test and compare the results from different samplers when estimating a Hierarchical Gaussian Mixture Model. The “workflow” is:</p> <ul> <li>Generate data from a Gaussian Mixture with known parameters (we do this for the different cases we are interested in: low/high overlap …)</li> <li>Use the Hierarchical Gaussian Mixture Model to estimate the parameters of the mixture which generated the data</li> <li>Compare the different samplers considered (in our case, <a href="https://arxiv.org/pdf/1111.4246" rel="external nofollow noopener" target="_blank">NUTS</a>, <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo" rel="external nofollow noopener" target="_blank">Hamiltonian Monte Carlo</a>, <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm" rel="external nofollow noopener" target="_blank">Metropolis-Hastings</a> and <a href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="external nofollow noopener" target="_blank">Gibbs sampling</a>) to determine which one gives the optimal results</li> </ul> <p>The project was implemented using <a href="https://www.pymc.io/welcome.html" rel="external nofollow noopener" target="_blank">PyMC</a>, which by default implements most samplers, but not Gibbs sampling. In this post, I’ll walk you through the <em>theoretical derivation of the Gibbs Sampler</em> for this model!</p> <p><em>Note: In this case, for simplicity, we assumed that the weights \(\pi\) in the mixture are known. Though the model can be easily extended to include a Dirichlet prior on the probability vector \(\pi\).</em></p> <h3 id="1-full-conditional">1. Full conditional</h3> <p>The model we are interested in is</p> \[\begin{align*} &amp; \mu_0 \sim \mathcal{N}(\theta, \nu) \\ &amp; \mu_k | \mu_0 \sim \mathcal{N}(\mu_0, \tau^2), \quad \text{$\tau^2$ known} \\ &amp; \sigma^2_k \sim \text{InvGamma}(\alpha, \beta) \quad \text{$\alpha$, $\beta$ known} \\ &amp; z_i \sim \text{Categorical}(\pi) \\ &amp; y_i | z_i = k, \mu_k, \sigma_k^2 \sim \mathcal{N}(\mu_k, \sigma^2_k) \end{align*}\] <p>Note that we use the term <em>hierarchical</em> as we added a <em>hyperprior</em> \(\mu_0\), which the means of the components \(\mu_k\) depend on.</p> <p>Which we can represent the model graphically as:</p> <p align="center"> <img src="/assets/img/2025-02-02-plot.png" width="60%" height="300px"> </p> <p>In order to use <a href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="external nofollow noopener" target="_blank">Gibbs sampling</a> we have to obtain the conditional distribution of each of the components in the model. Using the graphical representation is helpful in obtaining the conditional distributions of the different components.</p> <p><strong>1.1 Conditional for \(z_i\)</strong></p> <p>We have</p> \[\text{p}(z_i | y_i, \mu_k, \sigma^2_k, \mu_0) = \text{p}(z_i | y_i) = \frac{\text{p}(y_i | z_i) \text{p}(z_i)}{\text{p}(y_i)} = \frac{\pi_k \mathcal{N}(\mu_k, \sigma^2_k)}{\sum_{j = 1}^k \pi_j \mathcal{N}(\mu_j, \sigma^2_j)}\] <p><strong>1.2 Conditional for \(\mu_k\)</strong></p> <p>We denote \(\mu_{-k} = (\mu_1, \mu_2, \dots, \mu_{k-1}, \mu_{k+1}, \dots, \mu_n)\) as the vector of means with the \(k\)-th element removed and \(N_i = \#\{i: z_i = 1\}\), the number of observations such that \(z_i = 1\).</p> \[\begin{align*} \text{p}(\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i) &amp;= \text{p}(\mu_k | z_i, \mu_0, \sigma^2_k, y_i) \\ &amp; \propto \prod_{i: z_i = k} \text{p}(y_i | \mu_k, \sigma^2_k, \mu_0) \text{p}(\mu_k, \mu_0, \sigma^2_k) \\ &amp; \propto \text{p}(\mu_k | \mu_0) \prod_{i: z_i = k} \text{p}(y_i | \mu_k, \sigma^2_k) \\ &amp; \propto \text{exp}\left\{\underbrace{-\frac{1}{2 \tau^2} (\mu_k - \mu_0)^2 + \sum_{i: z_i = k} \frac{1}{2 \sigma_k^2} (y_i - \mu_k)^2}_{(*)} \right\} \end{align*}\] <p>We consider the expression in the last line inside the exponential, and expand it as</p> \[\begin{align*} (*) &amp;= \frac{\sum_{i: z_i = k} y_i^2}{2 \sigma^2_k} + \frac{N_i \mu_k^2}{2 \sigma_k^2} - \frac{\mu_k \sum_{i: y_i = k} y_i}{2 \sigma^2_k} - \frac{1}{2 \tau^2} \mu_k^2 - \frac{1}{2 \tau^2} \mu_0^2 + \frac{2}{\tau^2}{\mu_k\mu_0} \\ &amp;= \frac{1}{2} \left\{ \underbrace{\left( \frac{N_i}{\sigma_k^2} + \frac{1}{\tau^2} \right)}_{a} \mu_k^2 - 2 \underbrace{\left( \frac{1}{\tau^2} \mu_0 + \frac{\sum_{i: z_i = k}y_i}{\sigma_k^2} \right)}_{b} \mu_k + \underbrace{\frac{\sum_{i: z_i = k} y_i^2}{\sigma_k^2} - \frac{1}{\tau^2} \mu_0^2}_{c} \right\} \\ &amp;= \frac{1}{2} \{a \mu_k^2 - 2b \mu_k + c\} \\ &amp;= \frac{1}{2} \left\{ a \left( \mu_k^2 - 2 \frac{b}{a} \mu_k \textcolor{red}{\pm \frac{b^2}{a^2}} \right) + c \right\} \\ &amp;= - \frac{1}{2} \left\{ a \left( \left( \mu_k - \frac{b}{a}\right)^2 - \frac{b^2}{a^2} \right) + c \right\}. \end{align*}\] <p>So, we conclude that</p> \[\boxed{\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i \sim \mathcal{N}\left( \frac{b}{a}, \frac{1}{a} \right).}\] <p>This implies that</p> \[\begin{align*} &amp; \mathbb{E}[\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i] = \frac{b}{a} = \frac{\frac{N_i}{\tau^2} + \frac{\sum_{i: z_i = k} y_i}{\sigma_k^2}}{\frac{N_i}{\sigma_k^2} + \frac{1}{\tau^2}} \\ &amp; \text{Var}(\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i) = \frac{1}{a} = \frac{1}{\frac{N_i}{\sigma_k^2} + \frac{1}{\tau^2}}. \end{align*}\] <p>The main <em>“trick”</em> needed to obtain the result is simply adding and subtracting \(\frac{b}{a}\). I’ve personally found this idea to be extremely useful in cases similar to this, where we need to manipulate algebraic expressions in order to obtain the distribution “we are looking for”.</p> <p><strong>1.3 Conditional for \(\mu_0\)</strong></p> <p>As for \(\mu_0\), we have</p> \[\begin{align*} \text{p}(\mu_0 | \mu_k, \sigma^2_k, y_i, z_i) &amp;= \text{p}(\mu_0 | \mu_k) \\ &amp;\propto \text{p}(\mu_k | \mu_0) \text{p}(\mu_0) \\ &amp;\propto \text{exp}\left\{ -\frac{1}{2 \tau^2} \sum_{k = 1}^K (\mu_k - \mu_0)^2 - \frac{1}{2 \nu^2} (\mu_0 - \theta)^2 \right\}. \end{align*}\] <p>Using an analogous process as that used for \(\mu_k\), we can write</p> \[\text{p}(\mu_k | \mu_0) \text{p}(\mu_0) \propto \text{exp}\left\{-\frac{1}{2} \left(a \mu_0^2 + b \mu_0 + c \right) \right\},\] <p>where \(a = \frac{1}{\tau^2} + \frac{1}{\nu^2}\), \(b = \frac{\sum_{k = 1}^K \mu_k}{\tau^2} + \frac{\theta}{\nu^2}\) and \(c = \frac{\sum_{k = 1}^K \mu_k^2}{\tau} + \frac{\theta^2}{\nu^2}\).</p> <p>Considering the expression inside the exponential, we can write</p> \[a \mu_0^2 + b \mu_0 + c = a\left( \mu_0^2 + \frac{b}{a}\mu_0 \textcolor{red}{\pm \frac{b^2}{a^2}} \right) + c = a \left( \left( \mu_0 - \frac{b}{a} \right)^2 - \frac{b^2}{a^2} \right) + c.\] <p>Then, we have that</p> \[\boxed{\mu_0 | \mu_k, \sigma^2_k, y_i, z_i \sim \mathcal{N}\left(\frac{b}{a}, \frac{1}{a}\right).}\] <p>So, in terms of expected value and variance we have</p> \[\begin{align*} &amp; \mathbb{E}[\mu_0 | \mu_k, \sigma^2_k, y_i, z_i] = \frac{b}{a} = \frac{\frac{\sum_{k = 1}^K \mu_k}{\tau^2} + \frac{\theta}{\nu^2}}{\frac{1}{\tau^2} + \frac{1}{\nu^2}}, \\ &amp; \text{Var}(\mu_0 | \mu_k, \sigma^2_k, y_i, z_i) = \frac{1}{a} = \frac{1}{\frac{1}{\tau^2} + \frac{1}{\nu^2}}. \end{align*}\] <p><strong>1.4 Conditional for \(\sigma_k^2\)</strong></p> <p>As \(\sigma_k^2 \sim \text{InvGamma}(\alpha, \beta)\) we can write its density as</p> \[f(\sigma_k^2; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} (\sigma_k^2)^{-(\alpha + 1)} \text{exp}\left\{- \frac{\beta}{\sigma_k^2} \right\}.\] <p>We can write the condtional density as</p> \[\begin{align*} \text{p}(\sigma^2_k | \mu_k, \mu_0, y_i, z_i) &amp;\propto \prod_{i: z_i = k} \text{p}(y_i | \mu_k, \sigma^2_k) \text{p}(\sigma_k^2) \\ &amp;= (\sigma_k^2)^{-\left(\alpha_0 + 1 + \frac{N_k}{2} \right)} \text{exp} \left\{ - \frac{1}{\sigma_k^2} \left( \beta_0 + \frac{1}{2} \sum_{i: z_i = k} (y_i - \mu_k)^2 \right) \right\} \end{align*}\] <p>Letting \(\alpha' = \alpha_0 + \frac{N_k}{2}\) and \(\beta' = \beta_0 + \frac{1}{2} \sum_{i: z_i = k} (y_i - \mu_k)^2\), we have</p> \[\boxed{\sigma^2_k | \mu_k, \mu_0, y_i, z_i \sim \text{InvGamma}(\alpha', \beta').}\] <p>That’s it! We now have all the conditional distributions we need to implement the sampler. You can view the code <a href="https://github.com/alexcaldarone/gmm-mcmc/blob/main/src/utils/gibbs_sampler.py" rel="external nofollow noopener" target="_blank">here</a>.</p> <h3 id="2-some-thoughts">2. Some Thoughts</h3> <h4 id="on-pymc">On PyMC</h4> <p>Implementing the Gibbs sampler with PyMC proved relatively straightforward when dealing with the univariate model. All that was required was implementing a custom class which inherited from the BlockedStep step method contained in the PyMC library. This allows us to implement a custom sampler and let PyMC do the heavy lifting in terms of providing convergence statistics, saving the results, plotting the estimated distributions etc…</p> <p>When it came to implementing the sampler for the multivariate version of this model model (which extends the means \(\mu_k, \mu_0\) and uses a joint Normal-Inverse Wishart prior for \((\mu_k, \Sigma_k)\)), things started to break down. I still haven’t quite figured out why, but using PyMC resulted in degenerate situations in which all the observations are assigned to one component, the covariance matrix sampled is singular (even when using the <a href="https://www.pymc.io/projects/examples/en/latest/howto/LKJ.html" rel="external nofollow noopener" target="_blank">advised prior</a> and regularizing the terms), and other similar errors which made it extremely frustrating to work with PyMC. I’m still not quite sure whether these errors are due to the formulation of the model or some PyMC internals which I’m not too familiar with. Though frustrating it has been insightful in learning how PyMC works in more detail, though for the multivariate model I might turn to using a different framework or code the whole sampling process from scratch!</p> <h4 id="mixture-models">Mixture Models</h4> <p>While at first deriving all the full condtionals for these models can seem like a daunting task, once you have written down the graph representing the model most of the heavy lifting is done. At this point, all you really need to get your full conditionals are:</p> <ul> <li>Obviously, <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="external nofollow noopener" target="_blank">Bayes’ theorem!</a> </li> <li>Dropping any terms which do not depend on the parameter of interest</li> <li>Using the two above, recognizing the <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)#Bayesian_statistics" rel="external nofollow noopener" target="_blank">kernel</a> of known random variables to derive the actual full conditional distribution</li> <li>As in the case of \(\mu_0\) and \(\mu_k\), conveniently renaming some of the terms for ease of notation and, once we have an easier to interpret expression, using some good old algebra to try to obtain the expression we are looking for (in this case, completing the square and obtaining the Guassian kernel)</li> </ul> <hr> <p><em>I would like to thank <a href="https://www.thisiscrispin.com/" rel="external nofollow noopener" target="_blank">Dominique Paul</a> for reading the first version of this post and suggesting some useful improvements.</em></p> </div> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>