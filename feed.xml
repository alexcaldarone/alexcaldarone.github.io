<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://alexcaldarone.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alexcaldarone.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-23T19:36:41+00:00</updated><id>https://alexcaldarone.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">MCMC for Gaussian Mixture Models</title><link href="https://alexcaldarone.github.io/blog/2025/gmm-mcmc/" rel="alternate" type="text/html" title="MCMC for Gaussian Mixture Models"/><published>2025-02-02T00:00:00+00:00</published><updated>2025-02-02T00:00:00+00:00</updated><id>https://alexcaldarone.github.io/blog/2025/gmm-mcmc</id><content type="html" xml:base="https://alexcaldarone.github.io/blog/2025/gmm-mcmc/"><![CDATA[<p>Gaussian Mixture Models (GMMs) are incredibly versatile tools for solving real-world problems, from clustering and image analysis to speech processing. They enable us to model complex, multimodal data distributions effectively. Traditionally, the <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization algorithm</a> is used to estimate the parameters of these models.</p> <p>But what happens if we shift to a <em>Bayesian approach?</em> In a Bayesian setting, we gain the advantage of incorporating prior knowledge and quantifying uncertainty, but this comes at the cost of more complex computations. In order to compute the quantities of interest, we turn to <strong>Markov Chain Monte Carlo (MCMC)</strong> methods, which allow us to sample from the posterior distribution of the model’s parameters.</p> <p>Recently I implemented a completed a project on comparing different MCMC (Markov Chain Monte Carlo) samplers when estimating the parameters of Hierarchical Guassian Mixture Model, and thought I’d share some details! You can find the code of the project <a href="https://github.com/alexcaldarone/gmm-mcmc">here</a>. In a nutshell, it allows the user to test and compare the results from different samplers when estimating a Hierarchical Gaussian Mixture Model. The “workflow” is:</p> <ul> <li>Generate data from a Gaussian Mixture with known parameters (we do this for the different cases we are interested in: low/high overlap …)</li> <li>Use the Hierarchical Gaussian Mixture Model to estimate the parameters of the mixture which generated the data</li> <li>Compare the different samplers considered (in our case, <a href="https://arxiv.org/pdf/1111.4246">NUTS</a>, <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo</a>, <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis-Hastings</a> and <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a>) to determine which one gives the optimal results</li> </ul> <p>The project was implemented using <a href="https://www.pymc.io/welcome.html">PyMC</a>, which by default implements most samplers, but not Gibbs sampling. In this post, I’ll walk you through the <em>theoretical derivation of the Gibbs Sampler</em> for this model!</p> <p><em>Note: In this case, for simplicity, we assumed that the weights \(\pi\) in the mixture are known. Though the model can be easily extended to include a Dirichlet prior on the probability vector \(\pi\).</em></p> <h3 id="1-full-conditional">1. Full conditional</h3> <p>The model we are interested in is</p> \[\begin{align*} &amp; \mu_0 \sim \mathcal{N}(\theta, \nu) \\ &amp; \mu_k | \mu_0 \sim \mathcal{N}(\mu_0, \tau^2), \quad \text{$\tau^2$ known} \\ &amp; \sigma^2_k \sim \text{InvGamma}(\alpha, \beta) \quad \text{$\alpha$, $\beta$ known} \\ &amp; z_i \sim \text{Categorical}(\pi) \\ &amp; y_i | z_i = k, \mu_k, \sigma_k^2 \sim \mathcal{N}(\mu_k, \sigma^2_k) \end{align*}\] <p>Note that we use the term <em>hierarchical</em> as we added a <em>hyperprior</em> \(\mu_0\), which the means of the components \(\mu_k\) depend on.</p> <p>Which we can represent the model graphically as:</p> <p align="center"> <img src="/assets/img/2025-02-02-plot.png" width="60%" height="300px"/> </p> <p>In order to use <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a> we have to obtain the conditional distribution of each of the components in the model. Using the graphical representation is helpful in obtaining the conditional distributions of the different components.</p> <p><strong>1.1 Conditional for \(z_i\)</strong></p> <p>We have</p> \[\text{p}(z_i | y_i, \mu_k, \sigma^2_k, \mu_0) = \text{p}(z_i | y_i) = \frac{\text{p}(y_i | z_i) \text{p}(z_i)}{\text{p}(y_i)} = \frac{\pi_k \mathcal{N}(\mu_k, \sigma^2_k)}{\sum_{j = 1}^k \pi_j \mathcal{N}(\mu_j, \sigma^2_j)}\] <p><strong>1.2 Conditional for \(\mu_k\)</strong></p> <p>We denote \(\mu_{-k} = (\mu_1, \mu_2, \dots, \mu_{k-1}, \mu_{k+1}, \dots, \mu_n)\) as the vector of means with the \(k\)-th element removed and \(N_i = \#\{i: z_i = 1\}\), the number of observations such that \(z_i = 1\).</p> \[\begin{align*} \text{p}(\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i) &amp;= \text{p}(\mu_k | z_i, \mu_0, \sigma^2_k, y_i) \\ &amp; \propto \prod_{i: z_i = k} \text{p}(y_i | \mu_k, \sigma^2_k, \mu_0) \text{p}(\mu_k, \mu_0, \sigma^2_k) \\ &amp; \propto \text{p}(\mu_k | \mu_0) \prod_{i: z_i = k} \text{p}(y_i | \mu_k, \sigma^2_k) \\ &amp; \propto \text{exp}\left\{\underbrace{-\frac{1}{2 \tau^2} (\mu_k - \mu_0)^2 + \sum_{i: z_i = k} \frac{1}{2 \sigma_k^2} (y_i - \mu_k)^2}_{(*)} \right\} \end{align*}\] <p>We consider the expression in the last line inside the exponential, and expand it as</p> \[\begin{align*} (*) &amp;= \frac{\sum_{i: z_i = k} y_i^2}{2 \sigma^2_k} + \frac{N_i \mu_k^2}{2 \sigma_k^2} - \frac{\mu_k \sum_{i: y_i = k} y_i}{2 \sigma^2_k} - \frac{1}{2 \tau^2} \mu_k^2 - \frac{1}{2 \tau^2} \mu_0^2 + \frac{2}{\tau^2}{\mu_k\mu_0} \\ &amp;= \frac{1}{2} \left\{ \underbrace{\left( \frac{N_i}{\sigma_k^2} + \frac{1}{\tau^2} \right)}_{a} \mu_k^2 - 2 \underbrace{\left( \frac{1}{\tau^2} \mu_0 + \frac{\sum_{i: z_i = k}y_i}{\sigma_k^2} \right)}_{b} \mu_k + \underbrace{\frac{\sum_{i: z_i = k} y_i^2}{\sigma_k^2} - \frac{1}{\tau^2} \mu_0^2}_{c} \right\} \\ &amp;= \frac{1}{2} \{a \mu_k^2 - 2b \mu_k + c\} \\ &amp;= \frac{1}{2} \left\{ a \left( \mu_k^2 - 2 \frac{b}{a} \mu_k \textcolor{red}{\pm \frac{b^2}{a^2}} \right) + c \right\} \\ &amp;= - \frac{1}{2} \left\{ a \left( \left( \mu_k - \frac{b}{a}\right)^2 - \frac{b^2}{a^2} \right) + c \right\}. \end{align*}\] <p>So, we conclude that</p> \[\boxed{\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i \sim \mathcal{N}\left( \frac{b}{a}, \frac{1}{a} \right).}\] <p>This implies that</p> \[\begin{align*} &amp; \mathbb{E}[\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i] = \frac{b}{a} = \frac{\frac{N_i}{\tau^2} + \frac{\sum_{i: z_i = k} y_i}{\sigma_k^2}}{\frac{N_i}{\sigma_k^2} + \frac{1}{\tau^2}} \\ &amp; \text{Var}(\mu_k | \mu_{-k}, z_i, \mu_0, \sigma^2_k, y_i) = \frac{1}{a} = \frac{1}{\frac{N_i}{\sigma_k^2} + \frac{1}{\tau^2}}. \end{align*}\] <p>The main <em>“trick”</em> needed to obtain the result is simply adding and subtracting \(\frac{b}{a}\). I’ve personally found this idea to be extremely useful in cases similar to this, where we need to manipulate algebraic expressions in order to obtain the distribution “we are looking for”.</p> <p><strong>1.3 Conditional for \(\mu_0\)</strong></p> <p>As for \(\mu_0\), we have</p> \[\begin{align*} \text{p}(\mu_0 | \mu_k, \sigma^2_k, y_i, z_i) &amp;= \text{p}(\mu_0 | \mu_k) \\ &amp;\propto \text{p}(\mu_k | \mu_0) \text{p}(\mu_0) \\ &amp;\propto \text{exp}\left\{ -\frac{1}{2 \tau^2} \sum_{k = 1}^K (\mu_k - \mu_0)^2 - \frac{1}{2 \nu^2} (\mu_0 - \theta)^2 \right\}. \end{align*}\] <p>Using an analogous process as that used for \(\mu_k\), we can write</p> \[\text{p}(\mu_k | \mu_0) \text{p}(\mu_0) \propto \text{exp}\left\{-\frac{1}{2} \left(a \mu_0^2 + b \mu_0 + c \right) \right\},\] <p>where \(a = \frac{1}{\tau^2} + \frac{1}{\nu^2}\), \(b = \frac{\sum_{k = 1}^K \mu_k}{\tau^2} + \frac{\theta}{\nu^2}\) and \(c = \frac{\sum_{k = 1}^K \mu_k^2}{\tau} + \frac{\theta^2}{\nu^2}\).</p> <p>Considering the expression inside the exponential, we can write</p> \[a \mu_0^2 + b \mu_0 + c = a\left( \mu_0^2 + \frac{b}{a}\mu_0 \textcolor{red}{\pm \frac{b^2}{a^2}} \right) + c = a \left( \left( \mu_0 - \frac{b}{a} \right)^2 - \frac{b^2}{a^2} \right) + c.\] <p>Then, we have that</p> \[\boxed{\mu_0 | \mu_k, \sigma^2_k, y_i, z_i \sim \mathcal{N}\left(\frac{b}{a}, \frac{1}{a}\right).}\] <p>So, in terms of expected value and variance we have</p> \[\begin{align*} &amp; \mathbb{E}[\mu_0 | \mu_k, \sigma^2_k, y_i, z_i] = \frac{b}{a} = \frac{\frac{\sum_{k = 1}^K \mu_k}{\tau^2} + \frac{\theta}{\nu^2}}{\frac{1}{\tau^2} + \frac{1}{\nu^2}}, \\ &amp; \text{Var}(\mu_0 | \mu_k, \sigma^2_k, y_i, z_i) = \frac{1}{a} = \frac{1}{\frac{1}{\tau^2} + \frac{1}{\nu^2}}. \end{align*}\] <p><strong>1.4 Conditional for \(\sigma_k^2\)</strong></p> <p>As \(\sigma_k^2 \sim \text{InvGamma}(\alpha, \beta)\) we can write its density as</p> \[f(\sigma_k^2; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} (\sigma_k^2)^{-(\alpha + 1)} \text{exp}\left\{- \frac{\beta}{\sigma_k^2} \right\}.\] <p>We can write the condtional density as</p> \[\begin{align*} \text{p}(\sigma^2_k | \mu_k, \mu_0, y_i, z_i) &amp;\propto \prod_{i: z_i = k} \text{p}(y_i | \mu_k, \sigma^2_k) \text{p}(\sigma_k^2) \\ &amp;= (\sigma_k^2)^{-\left(\alpha_0 + 1 + \frac{N_k}{2} \right)} \text{exp} \left\{ - \frac{1}{\sigma_k^2} \left( \beta_0 + \frac{1}{2} \sum_{i: z_i = k} (y_i - \mu_k)^2 \right) \right\} \end{align*}\] <p>Letting \(\alpha' = \alpha_0 + \frac{N_k}{2}\) and \(\beta' = \beta_0 + \frac{1}{2} \sum_{i: z_i = k} (y_i - \mu_k)^2\), we have</p> \[\boxed{\sigma^2_k | \mu_k, \mu_0, y_i, z_i \sim \text{InvGamma}(\alpha', \beta').}\] <p>That’s it! We now have all the conditional distributions we need to implement the sampler. You can view the code <a href="https://github.com/alexcaldarone/gmm-mcmc/blob/main/src/utils/gibbs_sampler.py">here</a>.</p> <h3 id="2-some-thoughts">2. Some Thoughts</h3> <h4 id="on-pymc">On PyMC</h4> <p>Implementing the Gibbs sampler with PyMC proved relatively straightforward when dealing with the univariate model. All that was required was implementing a custom class which inherited from the BlockedStep step method contained in the PyMC library. This allows us to implement a custom sampler and let PyMC do the heavy lifting in terms of providing convergence statistics, saving the results, plotting the estimated distributions etc…</p> <p>When it came to implementing the sampler for the multivariate version of this model model (which extends the means \(\mu_k, \mu_0\) and uses a joint Normal-Inverse Wishart prior for \((\mu_k, \Sigma_k)\)), things started to break down. I still haven’t quite figured out why, but using PyMC resulted in degenerate situations in which all the observations are assigned to one component, the covariance matrix sampled is singular (even when using the <a href="https://www.pymc.io/projects/examples/en/latest/howto/LKJ.html">advised prior</a> and regularizing the terms), and other similar errors which made it extremely frustrating to work with PyMC. I’m still not quite sure whether these errors are due to the formulation of the model or some PyMC internals which I’m not too familiar with. Though frustrating it has been insightful in learning how PyMC works in more detail, though for the multivariate model I might turn to using a different framework or code the whole sampling process from scratch!</p> <h4 id="mixture-models">Mixture Models</h4> <p>While at first deriving all the full condtionals for these models can seem like a daunting task, once you have written down the graph representing the model most of the heavy lifting is done. At this point, all you really need to get your full conditionals are:</p> <ul> <li>Obviously, <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem!</a></li> <li>Dropping any terms which do not depend on the parameter of interest</li> <li>Using the two above, recognizing the <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)#Bayesian_statistics">kernel</a> of known random variables to derive the actual full conditional distribution</li> <li>As in the case of \(\mu_0\) and \(\mu_k\), conveniently renaming some of the terms for ease of notation and, once we have an easier to interpret expression, using some good old algebra to try to obtain the expression we are looking for (in this case, completing the square and obtaining the Guassian kernel)</li> </ul> <hr/> <p><em>I would like to thank <a href="https://www.thisiscrispin.com/">Dominique Paul</a> for reading the first version of this post and suggesting some useful improvements.</em></p>]]></content><author><name></name></author><category term="Statistics,"/><category term="Bayesian_Statistics"/><summary type="html"><![CDATA[Gaussian Mixture Models (GMMs) are incredibly versatile tools for solving real-world problems, from clustering and image analysis to speech processing. They enable us to model complex, multimodal data distributions effectively. Traditionally, the Expectation-Maximization algorithm is used to estimate the parameters of these models.]]></summary></entry><entry><title type="html">Lessons Learned From My First NLP Competition</title><link href="https://alexcaldarone.github.io/blog/2023/lessons-nlp-competition/" rel="alternate" type="text/html" title="Lessons Learned From My First NLP Competition"/><published>2023-09-19T00:00:00+00:00</published><updated>2023-09-19T00:00:00+00:00</updated><id>https://alexcaldarone.github.io/blog/2023/lessons-nlp-competition</id><content type="html" xml:base="https://alexcaldarone.github.io/blog/2023/lessons-nlp-competition/"><![CDATA[<p>Earlier this month, together with two other students from the University of Padua, I took part in the <a href="https://journal.opendataplayground.com/ITADATAhack-2023/">ITADATAhack 2023</a> competition. This proved to be an incredible opportunity to push my capabilities in data analysis and modelling.</p> <p>The competition consisted of developing a classification algorithm that, given the raw text of a legal document as input, would be able to classify the document in the correct category. In order to do this, we were given access to a dataset containing the legal documents found in the EURLex database. The competition spanned over 3 days and was organized in increasing levels of difficulty.</p> <p>On the first day we had to solve a 20 label classification problem, then a 96 label classification on the second day and then on the last day we had to deal with a binary multi-label classification problem (where each label had 89 dimensions). This competition was open to all universities in Italy and there were a total of 31 participating teams.</p> <h3 id="1-how-do-you-embed-text">1. How do you embed text?</h3> <p>After analysing and cleaning the raw text, one of the most important steps is <em>embedding</em> it. Thats is, to convert it from words to numbers. For this step we tried two different approaces: a simple one (TF-IDF) and a more complicated one (transformer model embeddings).</p> <p><strong><em>TF-IDF</em></strong></p> <p>The <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF</a> is a combination of two measures to assign a score to every word in a document. The first measure <em>tf</em> (term frequency) considers the frequency of a word inside of a document, while <em>idf</em> (inverse document frequency) is a measure of the importance of how much information a word provides (i.e. if it’s very common or rare among the documents in the corpus). The final score is the multiplication of these two measures.</p> <p>To our surprise the TF-IDF approach worked incredibly well, giving a classification accuracy of over 90% (on the dataset with balanced labels) when combined with a model such as logistic regression or KNN classification. Although this method has an interpretability advantage over the transformer model embeddings, it does come at the cost of having to perfrom computiationally intensive text cleaning (such as stopword removal, lemmatization of text and further custom cleaning procedures specifically for the type of text we were handling) in order to perform well. Overall, even if transformer model embeddings allowed us to achieve slightly better results, the TF-IDF approach proved to be very valid and how a simple solution can perform very well even on a complicated task.</p> <p><strong><em>Transformer model embeddings</em></strong></p> <p>All of our final solutions to the challenge utilized transformer model embeddings as the bedrock on which we built our models. Initially we debated whether to use a fine-tuned version of BERT (<a href="https://huggingface.co/nlpaueb/legal-bert-base-uncased">LEGAL-BERT</a>) or another model we had found on HuggingFace, which was <a href="https://huggingface.co/intfloat/e5-small-v2">e5-small</a>. After some tests we ended up going with the latter. This was because this model’s embedding dimension is 384, compared to 768 for the fine-tuned version of BERT. After some testing we established that even though LEGAL-BERT was finetuned on legal documents (including the EURLex database) it did not provide an increase in the accuracy of the models that were trained utilizing the word embeddings.</p> <p>Embedding long documents proved to be an interesting challenge. This was becuase the transformer model we chose had a maximum input size of 512 tokens, but we were dealing with documents that had varying length, with some being several thousand words long. We solved this problem by tokenizing and embedding the text using a “sliding window approach”, that is we only considered <em>n</em> words at a time, tokenized them and added them to an array where we stored the current law’s tokens. Then we iterated over this array and passed 512 tokens at a time to the transformer model in order to obtain their embeddings. Finally we concatenateed the embeddings of the various sequences into an array. This meant that we had embedding vectors of varying length (shorter laws had shorter embedding vectors, while longer laws had longer embedding vectors).</p> <p>This solution posed another challenge. As we know, when training neural networks the inputs all need to have the same dimension. We decided to solve this problem by averaging the elements in the embedding vector in order to obtain vectors that all had 384 dimensions (see <a href="https://datascience.stackexchange.com/questions/107462/why-does-averaging-word-embedding-vectors-exctracted-from-the-nn-embedding-laye">here</a> why it makes sense to average embedding vectors).</p> <p>Embedding vectors also had the advantage of allowing us to augment the dataset in order to create “fake data” to train models that would have otherwise struggled to classify the laws correctly because of unbalanced labels.</p> <p>Overall these embeddings proved to be an extremely powerful and versatile instrument that allowed us to train a variety of models efficiently, although this came at the cost of losing the interpretability we had when using the TF-IDF method and a higher computational cost (we had to use GPUs in order to obtain them in a time-efficient way).</p> <h3 id="2-challenges-of-unbalanced-datasets">2. Challenges of unbalanced datasets</h3> <p>On the first day of the competition we had a 20 label classification problem and a balanced dataset. On the second day, we had more labels to classify correctly but we also had an <em>unbalanced</em> dataset. This proved to an interesting challenge to confront because for some categories we only had a handful or one example. This obviously meant that the models struggled to learn the features of the laws in these categories and failed to classify them correctly.</p> <p><strong><em>The solution</em></strong></p> <p>In order to solve this problem we devised a particular resampling technique. The script we ended up writing essentialy resampled the embedding vectors to create new instances of the law embeddings belonging to a certain category. This solution was based on the intuition that the embeddings of the laws amongst the different categories would have different properties and, that by resampling them, we would be able to create new embedding vectors that had properties similar to the ones in the category from which the orignal one was selected. Essentially this was a way of applying data augmentation to text embeddings.</p> <p>This method only made sense with the transformer model embeddings as these encode the similarity of the vaious tokens within each other, while the tf-idf representation of the corpus is based on the frequency of the words inside the documents and in how many documents these words appear. This means that we obviously couldn’t have resampled these frequencies in order to create new “fake data”.</p> <h3 id="3-features--models">3. Features &gt; Models</h3> <p>In the first day of the competition the TF-IDF representation of the corpus combined with simple statistical models such as logistic regression, K-nearest neighbors classification provided very good results. Although these results were very good, they were slightly outperformed by the accuracy obtained using a simple feed-forward neural network. The best performing model though was obtained using the transformer model embeddings combined with additional features designed to capture another important features present in the data, which was the citations of each law (or, the other laws that a particular law referenced). One of these features was the label probability distribution for a particular law depending on its citations. We constructed this by considering the citations of a particular document and considering what was the number of documents in each category that referenced that shared at least on citation.</p> <p>For the second day the best performing model was a neural network which took as input the transformer mdoel embeddings, the citation features and some other features. In the second and third day the dataset rebalancing proved to be incredibly useful in increasing the model’s performance. The same architecture, with some small modifications to account for the different type of output, proved to be efficient for the third day as well.</p> <p>The scores our team obtained were:</p> <ul> <li>Day 1: 0.9236/1</li> <li>Day 2: 0.8681/1</li> <li>Day 3: 0.8542/1</li> </ul> <p>Final score (weighted average of the three): 0.8704/1</p> <p>The score for the first two days is the F-1 score, while the score utilized for the third day was the Jaccard score.</p> <p>Overall, while I am pleased with the result our team achieved I think there is one thing that penalized us: <em>not spending enough time on feature engineering</em>. After having obtained good baseline features such as the text embeddings, the features corresponding to the citations and some other, we shifted out attention to the models we were using. I think this penalized us and that, instead of spending too much time tweaking the model’s architecture, we should have spent most of our time exploring the raw data and trying to find other features which could have been more useful that the ones we had already found.</p> <p>This will be an incrediby important lesson that I will not forget when working on future projects. Even though it has been said many times, the best way to learn this lesson is to commit the mistake of focusing too much on the model instead of the data (<em>“garbage in, garbage out”</em> as they saying goes). <strong>Finding the correct features is more important than stressing over the model you are using.</strong></p> <hr/> <p>This competition was an incredible learning experience, where I managed to compress tons of learning and experimentation in just three days. While I do think that we commited some mistakes and could have done things differently, I learned a lot of useful lessons that I will carry with me in future projects. Our team placed 13th out of 31 participating teams, which I consider to be a good result given this was my first experience in this type of competition!</p>]]></content><author><name></name></author><category term="Programming"/><summary type="html"><![CDATA[Earlier this month, together with two other students from the University of Padua, I took part in the ITADATAhack 2023 competition. This proved to be an incredible opportunity to push my capabilities in data analysis and modelling.]]></summary></entry></feed>